{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this notebook we show how to use topic modelling for inferring latent topics that pervade the corpus of stories published by [Ruskin Bond](https://kv2libraipur.files.wordpress.com/2017/02/the-night-train-at-deoli-and-other-stories-ruskin-bond.pdf) over time using Latent Dirichlet Allocation. Based on the discovered topics we use topic modelling to shed light on interesting facts about the data .\n",
    "\n",
    "## Loading and vectorizing the corpus\n",
    "\n",
    "We prune words which absolute frequency in the corpus is less than 4, as well as words which relative frequency is higher than 80%, with the aim to only keep the most significant ones. Eventually, we build the vector space representation of these articles with $tf \\cdot idf$ weighting. It is a $n \\times m$ matrix denoted by $A$, where each line represents an article, with $n = 21$ (i.e. the number of articles) and $m = 614$ (i.e. the number of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tom_lib.structure.corpus import Corpus\n",
    "from tom_lib.visualization.visualization import Visualization\n",
    "\n",
    "corpus = Corpus(source_file_path='input/x.csv',\n",
    "                language='english',\n",
    "                vectorization='tfidf',\n",
    "                max_relative_frequency=0.8,\n",
    "                min_absolute_frequency=4)\n",
    "print('corpus size:', corpus.size)\n",
    "print('vocabulary size:', len(corpus.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the optimal number of topics ($k$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tom_lib.nlp.topic_model import NonNegativeMatrixFactorization,LatentDirichletAllocation\n",
    "topic_model = LatentDirichletAllocation(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Jaccard average stability\n",
    "\n",
    "The figure below shows this metric for a number of topics varying between 5 and 50 (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "output_notebook()\n",
    "\n",
    "p = figure(plot_height=250)\n",
    "p.line(range(10, 51), topic_model.greene_metric(min_num_topics=10, step=1, max_num_topics=50, top_n_words=10, tao=10), line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Kullback-Liebler divergence\n",
    "\n",
    "The figure below shows this metric for a number of topics varying between 5 and 50 (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_height=250)\n",
    "p.line(range(10, 51), topic_model.arun_metric(min_num_topics=10, max_num_topics=50, iterations=10), line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guided by the two metrics described previously, we manually evaluate the quality of the topics identified with $k$ varying between 12 and 20. Eventually, we judge that the best results are achieved with LDA for $k=12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =12\n",
    "topic_model.infer_topics(num_topics=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Description of the discovered topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "d = {'Most relevant words': [', '.join([word for word, weight in topic_model.top_words(i, 10)]) for i in range(k)]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we leverage the discovered topics to highlight interesting particularities about the EGC society. To be able to analyze the topics, supplemented with information about the related papers, we partition the papers into 15 non-overlapping clusters, i.e. a cluster per topic. Each article $i \\in [0;1-n]$ is assigned to the cluster $j$ that corresponds to the topic with the highest weight $w_{ij}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{cluster}_i = \\underset{j}{\\mathrm{argmax}}(w_{i,j})\n",
    "\\label{eq:cluster}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global topic proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=[str(_) for _ in range(k)], plot_height=350, x_axis_label='topic', y_axis_label='proportion')\n",
    "p.vbar(x=[str(_) for _ in range(k)], top=topic_model.topics_frequency(), width=0.7)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting attention, evolving interests\n",
    "\n",
    "Here we focus on topics topics 12 (social network analysis and mining) and 3 (association rule mining). The following figures describe these topics in terms of their respective top 10 words and top 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(topic_id):\n",
    "    words = [word for word, weight in topic_model.top_words(topic_id, 10)]\n",
    "    weights = [weight for word, weight in topic_model.top_words(topic_id, 10)]\n",
    "\n",
    "    p = figure(x_range=words, plot_height=300, plot_width=800, x_axis_label='word', y_axis_label='weight')\n",
    "    p.vbar(x=words, top=weights, width=0.7)\n",
    "    show(p)\n",
    "    \n",
    "def top_documents_df(topic_id):\n",
    "    top_docs = topic_model.top_documents(topic_id, 3)\n",
    "    d = {'Article title': [corpus.title(doc_id) for doc_id, weight in top_docs], 'Year': [int(corpus.date(doc_id)) for doc_id, weight in top_docs]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic #12\n",
    "\n",
    "##### Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_top_words(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 3 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_documents_df(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic #3\n",
    "##### Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 3 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_documents_df(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of the frequencies of topics 3 and 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_height=250, x_axis_label='year', y_axis_label='topic frequency')\n",
    "p.line(range(2015, 2020), [topic_model.topic_frequency(3, date=i) for i in range(2015, 2020)], line_width=2, line_color='blue', legend='topic #3')\n",
    "p.line(range(2015, 2020), [topic_model.topic_frequency(6, date=i) for i in range(2015, 2020)], line_width=2, line_color='red', legend='topic #12')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
